{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "import pandas as pd\n",
    "# Set the maximum number of rows and columns to display\n",
    "pd.set_option('display.max_rows', 200)  # Adjust this number as needed\n",
    "pd.set_option('display.max_columns', 50)  # Adjust this number as needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Assign CBGID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get shapefile of california"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get shapefiles\n",
    "import geopandas as gpd\n",
    "poly = gpd.GeoDataFrame.from_file('../../data_CityEvent/Shp/US_blck_grp_2019.shp')\n",
    "poly = poly.to_crs(epsg=4326)\n",
    "california = poly[poly['STATEFP']=='06']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CBGID assign function + Handle different geometry: Point, Polygon, and MultiPolygon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import json\n",
    "from shapely.geometry import shape\n",
    "\n",
    "def parse_geometry(geo):\n",
    "    \"\"\"Parse the GEO field into a Shapely geometry (Point, Polygon, or MultiPolygon).\"\"\"\n",
    "    if pd.isna(geo):\n",
    "        return None\n",
    "    try:\n",
    "        # Convert the GEO string into a Python dict\n",
    "        geo_json = json.loads(geo)\n",
    "\n",
    "        # Use shapely's shape() to parse the geometry\n",
    "        geometry = shape(geo_json)\n",
    "\n",
    "        # Ensure the parsed geometry is one of the expected types\n",
    "        if geometry.geom_type in {\"Point\", \"Polygon\", \"MultiPolygon\"}:\n",
    "            return geometry\n",
    "        else:\n",
    "            print(f\"Unsupported geometry type: {geometry.geom_type}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing geometry: {e}\")\n",
    "        return None\n",
    "\n",
    "def assign_cbgid(df, shapefile):\n",
    "    df['geometry'] = df['GEO'].apply(parse_geometry)\n",
    "    df = df.dropna(subset=['geometry'])\n",
    "    df = gpd.GeoDataFrame(df, geometry='geometry', crs='EPSG:4326')\n",
    "    shapefile = gpd.GeoDataFrame(shapefile, geometry='geometry', crs=\"EPSG:4326\")\n",
    "    shapefile = shapefile[['GEOID', 'geometry']]\n",
    "\n",
    "    assigned_df = gpd.sjoin(df, shapefile, how='inner', predicate='intersects')\n",
    "    print(df.shape)\n",
    "    print(assigned_df.shape)\n",
    "    assigned_df = assigned_df.drop(columns=['index_right'])\n",
    "    print(assigned_df.shape)\n",
    "    return(assigned_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save data by event category in california only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save function\n",
    "def save_df_by_event_category(df, event_type='event_type'):\n",
    "    columns = [col for col in df.columns]\n",
    "    categories = list(df['CATEGORY'].unique())\n",
    "    for category in categories:\n",
    "        df_by_category = df[df['CATEGORY'] == category]\n",
    "        print(f'{category} shape: {df_by_category.shape}')\n",
    "        df_by_category.to_csv(f'../../data_CityEvent/processed/1_cbgid_assigned_by_category/{event_type}{category}.csv', index=False)  # Set index=False to avoid saving the index\n",
    "    \n",
    "    print('Successful')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for handling cancelled/postponed events data + CBGID assigner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.time.time_converted_to_boston import convert_time_of_dataframe\n",
    "\n",
    "def dropping_rows_and_cbgid_assigner(df):\n",
    "    df = df[(df['CANCELLED_DT'].isna()) & (df['POSTPONED_DT'].isna())]\n",
    "\n",
    "    df = df.drop(['CANCELLED_DT', 'POSTPONED_DT'], axis=1)\n",
    "\n",
    "    datetime_columns = ['CREATE_DT', 'UPDATE_DT', 'EVENT_START', 'EVENT_END', 'PREDICTED_END', 'ROW_INSERTED_DT', 'ROW_UPDATED_DT']\n",
    "    df[datetime_columns] = df[datetime_columns].apply(pd.to_datetime, errors='coerce')\n",
    "\n",
    "    df_converted = convert_time_of_dataframe(df)\n",
    "\n",
    "    df_cbgid = assign_cbgid(df_converted, california)\n",
    "\n",
    "    save_df_by_event_category(df_cbgid)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attended Events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_attended = pd.read_csv('../../data_CityEvent/Cityevents/Demand_Intelligence_for_Attended_Events_California-0.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save attended\n",
    "dropping_rows_and_cbgid_assigner(df_attended, 'attended')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-Attended Events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_non_attended = pd.read_csv('../../data_CityEvent/Cityevents/Demand_Intelligence_for_Non_Attended_Events_California-0.csv')\n",
    "df_non_attended.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save non-attended\n",
    "dropping_rows_and_cbgid_assigner(df_non_attended, 'non_attended')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unscheduled Events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unscheduled = pd.read_csv('../../data_CityEvent/Cityevents/Demand_Intelligence_for_Unscheduled_Events_North_America-0.csv')\n",
    "df_unscheduled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save function\n",
    "def save_df_by_event_category_intermediate(df):\n",
    "\n",
    "    categories = list(df['CATEGORY'].unique())\n",
    "\n",
    "    for category in categories:\n",
    "        df_by_category = df[df['CATEGORY'] == category]\n",
    "        print(f'{category} shape: {df_by_category.shape}')\n",
    "        df_by_category.to_csv(f'../../data_CityEvent/processed/1_cbgid_assigned_by_category/unscheduled_intermediate_{category}.csv', index=False)  \n",
    "        # Set index=False to avoid saving the index\n",
    "    print('Successful')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_df_by_event_category_intermediate(df_unscheduled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "filepath = '../../data_CityEvent/processed/1_cbgid_assigned_by_category/unscheduled_intermediate'\n",
    "files = os.listdir(filepath)\n",
    "# print(files)\n",
    "\n",
    "for file in files:\n",
    "    df = pd.read_csv(f'../../data_CityEvent/processed/1_cbgid_assigned_by_category/unscheduled_intermediate/{file}')\n",
    "    print(file)\n",
    "    print('intermediate shape': df.shape)\n",
    "    df_assigned = assign_cbgid(df, california)\n",
    "    print('shape after assigning': df_assigned.shape)\n",
    "    df_assigned.to_csv(f'../../data_CityEvent/processed/1_cbgid_assigned_by_category/{file}', index=False) \n",
    "    print(f'{file} Save Successful')   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downscale the CBGID assigned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = '../../data_CityEvent/processed/1_cbgid_assigned_by_category'\n",
    "file_names = os.listdir(folder_path)\n",
    "for file_name in file_names:\n",
    "    print(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# downscale\n",
    "columns_to_keep = [\n",
    "    'EVENT_ID',\n",
    "    'CATEGORY',\n",
    "    'GEO',\n",
    "    'EVENT_START_BOSTON',\n",
    "    'EVENT_END_BOSTON',\n",
    "    'geometry',\n",
    "    'GEOID'\n",
    "]\n",
    "\n",
    "for file_name in file_names:\n",
    "    df = pd.read_csv(f'../../data_CityEvent/processed/1_cbgid_assigned_by_category/{file_name}')\n",
    "    print(file_name, df.shape)\n",
    "    df = df[columns_to_keep]\n",
    "    print(file_name, df.shape)\n",
    "    df.to_csv(f'../../data_CityEvent/processed/2_cbgid_downscaled/{file_name}', index=False)\n",
    "    print('Saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = '../../data_CityEvent/processed/2_cbgid_downscaled/'\n",
    "# List all files in the folder\n",
    "file_names = os.listdir(folder_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def hour_rows_df(df):\n",
    "    # Convert the EVENT_START_BOSTON and EVENT_END_BOSTON to datetime\n",
    "    datetime_columns = ['EVENT_START_BOSTON', 'EVENT_END_BOSTON']\n",
    "    for datetime_column in datetime_columns:\n",
    "        # Convert the specified columns to datetime format\n",
    "        df[datetime_column] = pd.to_datetime(df[datetime_column], errors='coerce')\n",
    "\n",
    "    columns_to_keep = ['EVENT_ID', 'GEOID', 'geometry', 'EVENT_START_BOSTON', 'EVENT_END_BOSTON', 'CATEGORY']\n",
    "    df = df[columns_to_keep]\n",
    "\n",
    "    # Define a function to create a range of hourly timestamps between start and end times\n",
    "    def create_hourly_rows(row):\n",
    "        start_time = row['EVENT_START_BOSTON'].floor('H')  # Round down to nearest hour\n",
    "        end_time = row['EVENT_END_BOSTON'].ceil('H')  # Round up to nearest hour\n",
    "        return pd.date_range(start=start_time, end=end_time, freq='H')\n",
    "\n",
    "    # Apply the function to each row and store the result in a new column 'hourly_times'\n",
    "    df['hourly_times'] = df.apply(create_hourly_rows, axis=1)\n",
    "\n",
    "    # Explode the DataFrame to create one row per hour\n",
    "    expanded_df = df.explode('hourly_times')\n",
    "\n",
    "    return expanded_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_name in file_names:\n",
    "    # if file_name[0] == 'n' or file_name[0] == 'u':\n",
    "    #     continue\n",
    "    current_df = pd.read_csv(f'{folder_path}/{file_name}')\n",
    "    print(f'{file_name}: {current_df.shape}')\n",
    "    datetime_columns = ['EVENT_START_BOSTON', 'EVENT_END_BOSTON']\n",
    "    for datetime_column in datetime_columns:\n",
    "        # Convert the specified columns to datetime format\n",
    "        current_df[datetime_column] = pd.to_datetime(current_df[datetime_column], errors='coerce')\n",
    "    \n",
    "    # Drop rows with NaN (or NaT) in any of the datetime columns\n",
    "    current_df.dropna(subset=datetime_columns, inplace=True)\n",
    "    \n",
    "    # Check the shape after dropping rows with NaT values\n",
    "    print(f'{file_name} after dropping NaT: {current_df.shape}')\n",
    "\n",
    "    current_df_hourly = hour_rows_df(current_df)    \n",
    "    file_name = file_name[:-4]\n",
    "    current_df_hourly.to_csv(f'../../data_CityEvent/processed/3_hourly_events_cbgid_category/{file_name}_hourly.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
